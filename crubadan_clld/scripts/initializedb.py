from __future__ import unicode_literals
import sys

from clld.scripts.util import initializedb, Data
from clld.db.meta import DBSession
from clld.db.models import common

import crubadan_clld
from crubadan_clld import models

import os
import codecs
from path import Path


# These are the files that will be lifted from the
# root crubadan directory and packaged into zip
# files for each language.
#
# format: [(NAME_IN_DATA_DIR, NAME_IN_ZIP_FILE)]
# 
# The files that go in the zip file automatically
# get the relevant language code prepended
# (e.g. "eng-testdata.txt")
#
packageFiles = [('TESTDATA', 'testdata.txt'),
                ('SOMETHING', 'something.txt')]

rootDataDir = '/data/crubadan'
rootClldDir = '/data/crubadan-clld'


def prepSysDirs():
    # os.system('rm -rf /data/crubadan-clld/*')
    os.system('if [ ! -d "' + rootClldDir + '" ]; then mkdir ' + rootClldDir + '; fi')
    os.system('if [ ! -d "' + rootClldDir + '/files" ]; then mkdir ' + rootClldDir + '/files; fi')

def fillTable(dbsession):
    langs = os.listdir(rootDataDir)
    c = 1
    for lang in langs:
        fname = rootDataDir + '/' + lang + '/' + 'EOLAS'
        mname = rootClldDir + '/metadata/' + lang + '.txt'
        trigfname = rootDataDir + '/' + lang + '/' + 'SAMPSENTS'
        if (os.path.isfile(fname)):
            f = codecs.open(fname, encoding='utf-8')
            fm = codecs.open(mname, encoding='utf-8')
            t = codecs.open(trigfname, encoding='utf-8')
            dic = {}

            # Read all ordinary data fields
            for line in f:
                parseAdd(line,dic,'')

            # Read all codata fields generated by that other script
            for line in fm:
                parseAdd(line,dic,'m_')

            # Add the path to the (not-yet-existant) dist zipfile to
            # the database
            dfile = models.WritingSystem_files(
                pk = lang,
                id = lang,
                name = lang,
                description = lang,
            )

            # Create the dist zipfile and store it in the right place
            z = lang + '.zip'
            os.system('mkdir ' + lang)
            os.system('cp doc/zip_file_LICENSE ' + lang + '/LICENSE')
            for (sysFile,zipFile) in packageFiles:
                qSysFile = rootDataDir + '/' + lang + '/' + sysFile
                qZipFile = lang + '/' + lang + '-' + zipFile
                os.system('cp ' + qSysFile + ' ' + qZipFile)
            os.system('zip -qr ' + z + ' ' + lang)
            os.system('mv ' + z + ' ' + rootClldDir + '/files/' + z)
            os.system('rm -r ' + lang)

            # Fill the database model
            ws = models.WritingSystem(

                # System stuff
                pk = lang,
                id = lang,
                jsondata = dic,
                name = dic[u'name_english'],
                description = dic[u'classification'],

                # Main data file
                eng_name = dic[u'name_english'],
                native_name = dic[u'name_native'],
                bcp47 = lang,
                iso6393 = dic[u'ISO_639-3'],
                country = dic[u'country'],
                script = dic[u'script'],
                parent_ws = dic[u'parent'],
                child_ws = dic[u'children'],
                ling_classification = dic[u'classification'],
                ethnologue_name = dic[u'ethnologue'],
                glottolog_name = dic[u'glottolog'],

                # Secondary data file ("metadata")
                #
                # I realize that this could probably be automated in
                # some way.. please don't laugh
                ## m_zip_size = dic[u'm_zip_size'],
                ## m_documents_crawled = dic[u'm_documents_crawled'],
                ## m_words = dic[u'm_words'],
                ## m_ethnologue_link = dic[u'm_ethnologue_link'],
                ## m_glottolog_link = dic[u'm_glottolog_link'],
                ## m_olac_link = dic[u'm_olac_link'],
                ## m_phoible_link = dic[u'm_phoible_link'],
                ## m_unesco_link = dic[u'm_unesco_link'],
                ## m_tweets_link = dic[u'm_tweets_link'],
                ## m_blogs_link = dic[u'm_blogs_link'],
                ## m_wikipedia_link = dic[u'm_wikipedia_link'],
                ## m_bible_link = dic[u'm_bible_link'],
                ## m_udhr_link = dic[u'm_udhr_link'],
                ## m_jw_link = dic[u'm_jw_link'],
                ## m_sample_link = dic[u'm_sample_link'],
            )

            dfile.object = ws
            
            dbsession.add(dfile)
            dbsession.add(ws)
    
            print 'Added ' + lang + ' ...'
            c += 1

def parseAdd(line,dic,prefix):
    if (line[0] != u'#'):
        (key,d,value) = line.partition(u' ')
        p_key = prefix + key
        if (value == u"XXX\n"):
            dic[p_key] = u"(Unknown)\n"
        elif ((value == u"none\n") or (value == u"\n")):
            dic[p_key] = u"None\n"
        else:
            dic[p_key] = value

def main(args):
    data = Data()

    dataset = common.Dataset(id=crubadan_clld.__name__, domain='crubadan_clld.clld.org')
    DBSession.add(dataset)

    fillTable(DBSession)

    # DBSession.add(models.WritingSystem(
    #     id=1,
    #     name='test_entry',
    #     description='hello testing',
    #     eng_name='Abau',
    #     bcp47='aau',
    #     iso6393='aau',
    #     country='Papua New Guinea',
    #     script='Latin',
    # ))

    # DBSession.commit()


def prime_cache(args):
    """If data needs to be denormalized for lookup, do that here.
    This procedure should be separate from the db initialization, because
    it will have to be run periodiucally whenever data has been updated.
    """


if __name__ == '__main__':
    prepSysDirs()
    initializedb(create=main, prime_cache=prime_cache)
    sys.exit(0)
